# LLM Tuning Hub 

<img src="images/repo-main.png" width="768" height="468"/>

If the last 6 months of AI research felt like a decade to you, you are not alone! With a new Large Language Model (LLM) released every other week, it has been challenging for us to keep up with the current pace of innovation in AI. While there has been a flurry of blog posts, tweets and code snippets on social media showcasing the power of LLMs and how to set up chat applications using them, we have not seen efforts that stress-test them for real-life business use-cases.

We, at Georgian, are moving fast to experiment with both open-source and closed-source LLMs. For a holistic evaluation, we will make use of the __Evaluation Framework__ that contains __4 pillars__:

- <img src="images/rocket.gif" width="32" height="32"/> Performance <img src="images/rocket.gif" width="32" height="32"/>
- <img src="images/time.gif" width="32" height="32"/> Time to Train <img src="images/time.gif" width="32" height="32"/>
- <img src="images/money.gif" width="32" height="32"/> Cost to Train <img src="images/money.gif" width="32" height="32"/>
- <img src="images/progress.gif" width="32" height="32"/> Inferencing <img src="images/progress.gif" width="32" height="32"/>

## What's in it for you?

For each of the above 4 pillars, we are sharing our codebase and insights to:
- Assist you to leverage LLMs for your business needs and challenges
- Decide which LLM best suites your needs from a performance and cost perspective
- Boost reproducibility efforts that are becoming increasingly difficult with LLMs

We are providing scripts that are ready-to-use for:
- Finetuning LLMs on your proprietary dataset via PeFT methodologies such as LoRA and Prefix Tuning!
- Performing hyperparameter optimization to get the maximum performance out of these models!

## What's the best way to use this repository?

Go over to the LLM-specific directory that you are interested in, and go over the ```README.md```. We have elaborated everything you need to know about the LLM, followed by performance results on open-source datasets!

## Roadmap

We are trying our best to perform extensive experiments on all LLMs. To that end, this is a tentative roadmap of LLMs that we aim to cover in the future:

- [x] Flan-T5
- [ ] Falcon (landing very soon!)
- [ ] RedPajama
- [ ] Llama-2
- [ ] OpenLlama
- [ ] SalesForce XGen
- [ ] OpenAI GPT-4
- [ ] Google PaLM
- [ ] Inflection Pi

## Correspondence

If you have any questions or issues, or would like to contribute to this repository, please reach out to:

- Rohit Saha ([Email](rohit@georgian.io) | [LinkedIn](httpg/www.linkedin.com/in/rohit-saha-ai/))
- Kyryl Truskovskyi ([Email](kyryl@georgian.io) | [LinkedIn](httpg/ca.linkedin.com/in/kyryl-truskovskyi-275b7967))
- Maria Ponomarenko ([Email](mariia.ponomarenko@georgian.io) | [LinkedIn](httpg/ca.linkedin.com/in/maria-ponomarenko-71b465179))

If you found this repository helpful, please leave a :star:. It helps ensure the AI research community can also benefit from & contribute to this repository!
